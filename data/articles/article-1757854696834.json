{
  "id": "article-1757854696834",
  "title": "Universal Process Manager",
  "title.en": "Universal Process Manager",
  "date": "2025-09-14",
  "description": "Robustní systém pro správu procesů s automatickým restartováním, monitoringem zdraví a čistým oddělením od spravovaných aplikací.",
  "description.en": "A robust, production-ready process management system with automatic restarts, health monitoring, and clean separation from managed applications.",
  "tags": [
    "process-manager",
    "python",
    "sqlite",
    "architektura",
    "monitoring",
    "zdravi-aplikaci"
  ],
  "tags.en": [
    "process-manager",
    "python",
    "sqlite",
    "architecture",
    "monitoring",
    "health-checks"
  ],
  "category": "tutorial, technologie",
  "category.en": "tutorial, technology",
  "markdown": "# Universal Process Manager: Komplexní technická analýza a implementace\n\n🔗 **GitHub Repository**: [https://github.com/hezky/lab_heartbeat](https://github.com/hezky/lab_heartbeat)\n\n> ⚠️ **Upozornění**: Tento článek popisuje aktuální stav aplikace k datu vytvoření. Implementační detaily a ukázky kódu se mohou v průběhu času měnit s vývojem projektu. Důležité jsou především představené koncepty, architektonické principy a myšlenky, které zůstávají platné bez ohledu na konkrétní implementaci.\n\n## TL;DR - Co získáte přečtením tohoto článku\n\n- **Pochopení architektury** moderního process manageru postaveného na principech SOLID a clean architecture\n- **Praktické ukázky** implementace klíčových komponent (Registry, Controller, Monitor, Heartbeat)\n- **Best practices** pro oddělení odpovědností mezi správcem procesů a aplikacemi\n- **Řešení reálných problémů** jako jsou restart politiky, health checks a resource limits\n- **Návrhové vzory** v praxi - Singleton, Repository, State Machine\n- **Testovací strategie** a optimalizační techniky pro vysoký výkon\n\n## Proč jsme postavili Universal Process Manager\n\n### Motivace projektu\n\nV produkčním prostředí jsme se opakovaně setkávali s několika problémy:\n\n1. **Systemd je příliš komplexní** pro jednoduché use cases a vyžaduje root přístup\n2. **PM2 je úzce svázán s Node.js** ekosystémem a má zbytečný overhead\n3. **Supervisor vyžaduje složitou konfiguraci** a neposkytuje moderní monitoring\n4. **Docker/Kubernetes je overkill** pro menší projekty a lokální development\n\n### Naše řešení vs. existující alternativy\n\n| Vlastnost            | Universal PM | systemd | PM2 | supervisor |\n|----------------------|--------------|---------|-----|------------|\n| Zero-dependency apps | ✅           | ✅      | ❌  | ✅         |\n| Bez nutnosti root    | ✅           | ❌      | ✅  | ✅         |\n| Multi-language       | ✅           | ✅      | ⚠️  | ✅         |\n| Health checks        | ✅           | ⚠️      | ✅  | ❌         |\n| REST API             | 🔄           | ❌      | ✅  | ❌         |\n| Resource limits      | ✅           | ✅      | ⚠️  | ❌         |\n| Jednoduchost         | ✅           | ❌      | ⚠️  | ⚠️         |\n\n## Architektura systému - vizuální přehled\n\n```\n┌──────────────────────────────────────────────────────────────┐\n│                         CLI Interface                         │\n│                    (Click Framework + Rich)                   │\n└────────────────────────────┬─────────────────────────────────┘\n                             │\n                             ▼\n┌──────────────────────────────────────────────────────────────┐\n│                      Core Components                          │\n│  ┌──────────┐  ┌──────────┐  ┌─────────┐  ┌──────────────┐ │\n│  │ Registry │◄─┤Controller│◄─┤ Monitor │◄─┤  Heartbeat   │ │\n│  │ (SQLite) │  │(Lifecycle)│  │(Health) │  │  (Real-time) │ │\n│  └──────────┘  └──────────┘  └─────────┘  └──────────────┘ │\n└──────────────────────────────────────────────────────────────┘\n                             │\n                             ▼\n┌──────────────────────────────────────────────────────────────┐\n│                    Managed Applications                       │\n│  ┌─────────────┐  ┌─────────────┐  ┌───────────────────┐   │\n│  │ Python App  │  │ Node.js App │  │   Shell Script    │   │\n│  │  (Flask)    │  │  (Express)  │  │   (Bash/Shell)    │   │\n│  └─────────────┘  └─────────────┘  └───────────────────┘   │\n└──────────────────────────────────────────────────────────────┘\n```\n\n### Životní cyklus procesu - State Machine\n\n```\n    ┌──────────────┐\n    │  REGISTERED  │\n    └──────┬───────┘\n           │ start\n           ▼\n    ┌──────────────┐\n    │   STARTING   │\n    └──────┬───────┘\n           │ success\n           ▼\n    ┌──────────────┐     crash/exit\n    │   RUNNING    │─────────────────┐\n    └──────┬───────┘                 │\n           │ stop                     ▼\n           ▼                  ┌──────────────┐\n    ┌──────────────┐          │   CRASHED    │\n    │   STOPPING   │          └──────┬───────┘\n    └──────┬───────┘                 │ restart\n           │ success                  │ (if policy)\n           ▼                          │\n    ┌──────────────┐                 │\n    │   STOPPED    │◄────────────────┘\n    └──────────────┘\n```\n\n## Úvod do problematiky správy procesů\n\nV moderním světě softwarového inženýrství představuje efektivní správa procesů jeden z klíčových pilířů stabilních a škálovatelných systémů. Universal Process Manager, známý také jako Lab Heartbeat, představuje inovativní přístup k řešení této komplexní problematiky. Tento článek poskytuje detailní technickou analýzu použitých technologií, architektonických rozhodnutí a implementačních metod tohoto systému.\n\nSpráva procesů není triviální záležitostí. Každý operační systém poskytuje základní nástroje pro spouštění a ukončování procesů, ale v produkčním prostředí potřebujeme mnohem více – monitoring, automatické restartování, health checks, centralizovanou správu logů a především spolehlivost. Universal Process Manager adresuje všechny tyto požadavky elegantním a modulárním způsobem.\n\n## Architektura systému a její filozofie\n\n### Vrstvená architektura\n\nUniversal Process Manager je postaven na principu striktního oddělení zodpovědností. Systém se skládá z několika klíčových vrstev, které spolu komunikují prostřednictvím dobře definovaných rozhraní:\n\n**Core vrstva** představuje srdce systému. Obsahuje čtyři hlavní komponenty:\n- Registry pro správu metadat procesů\n- Controller pro řízení životního cyklu\n- Monitor pro sledování stavu procesů\n- Heartbeat systém pro real-time monitoring\n\n**CLI vrstva** poskytuje uživatelské rozhraní prostřednictvím příkazové řádky. Využívá framework Click pro elegantní zpracování příkazů a Rich pro formátovaný výstup.\n\n**Perzistentní vrstva** zajišťuje ukládání dat pomocí SQLite databáze, která poskytuje dostatečný výkon pro lokální deployment při zachování jednoduchosti.\n\n### Principy oddělení\n\nKlíčovým architektonickým rozhodnutím bylo kompletní oddělení Process Manageru od spravovaných aplikací. Tento přístup přináší několik zásadních výhod:\n\n1. **Nezávislost aplikací** - Aplikace nemusí být modifikovány pro integraci s Process Managerem\n2. **Univerzalita** - Systém podporuje jakýkoliv typ procesu (Python, Node.js, Shell skripty, binární soubory)\n3. **Bezpečnost** - Process Manager nemůže poškodit aplikační kód a naopak\n4. **Flexibilita** - Snadné přidávání nových typů procesů bez změny existujícího kódu\n\n## Technologický stack a jeho komponenty\n\n### Python jako základ\n\nPython 3.8+ byl zvolen jako primární jazyk z několika důvodů:\n\n**Multiplatformnost** - Python běží spolehlivě na všech hlavních operačních systémech (Linux, macOS, Windows). Díky standardní knihovně subprocess můžeme spouštět procesy jednotným způsobem napříč platformami.\n\n**Bohatý ekosystém** - Využíváme několik klíčových knihoven:\n- `psutil` pro pokročilý monitoring procesů (CPU, paměť, síťové spojení)\n- `click` pro vytvoření profesionálního CLI rozhraní\n- `rich` pro formátovaný a barevný výstup v terminálu\n- `requests` pro HTTP health checks\n- `flask` pro demonstrační aplikace\n\n**Asynchronní schopnosti** - Ačkoliv hlavní implementace používá threading, Python umožňuje snadný přechod na asyncio pro ještě efektivnější správu velkého počtu procesů.\n\n### SQLite databáze\n\nSQLite představuje ideální volbu pro lokální perzistenci dat:\n\n**Serverless architektura** - Databáze běží přímo v procesu aplikace, což eliminuje nutnost správy dalšího serveru. To zjednodušuje deployment a snižuje systémové nároky.\n\n**ACID compliance** - SQLite garantuje atomicitu, konzistenci, izolaci a trvanlivost transakcí, což je kritické pro spolehlivou správu stavu procesů.\n\n**Výkon** - Pro náš use case (stovky až tisíce procesů) poskytuje SQLite více než dostatečný výkon. Využíváme prepared statements a connection pooling pro optimalizaci.\n\n**Schema databáze** je navrženo pro efektivní ukládání a vyhledávání:\n```sql\nCREATE TABLE processes (\n    id TEXT PRIMARY KEY,\n    name TEXT UNIQUE NOT NULL,\n    config TEXT NOT NULL,  -- JSON serializovaná konfigurace\n    state TEXT NOT NULL,\n    pid INTEGER,\n    started_at TEXT,\n    stopped_at TEXT,\n    restart_count INTEGER DEFAULT 0,\n    last_heartbeat TEXT,\n    error_message TEXT,\n    created_at TEXT DEFAULT CURRENT_TIMESTAMP,\n    updated_at TEXT DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE INDEX idx_state ON processes(state);\nCREATE INDEX idx_name ON processes(name);\nCREATE INDEX idx_heartbeat ON processes(last_heartbeat);\n```\n\n### Click framework pro CLI\n\nClick představuje moderní přístup k vytváření command-line interfaces:\n\n**Dekorátory** umožňují elegantní definici příkazů:\n```python\n@click.command()\n@click.option('--name', required=True, help='Process name')\n@click.option('--type', type=click.Choice(['python', 'nodejs', 'shell']))\ndef register(name, type):\n    \"\"\"Register a new process\"\"\"\n    pass\n```\n\n**Automatická validace** vstupů šetří mnoho kódu pro kontrolu parametrů.\n\n**Skupiny příkazů** umožňují logickou organizaci funkcionalit.\n\n### Rich pro formátovaný výstup\n\nRich knihovna transformuje způsob, jakým prezentujeme informace uživateli:\n\n**Tabulky** pro přehledné zobrazení stavu procesů:\n```python\ntable = Table(title=\"Process Status\")\ntable.add_column(\"Name\", style=\"cyan\")\ntable.add_column(\"State\", style=\"green\")\ntable.add_column(\"CPU %\", style=\"yellow\")\ntable.add_column(\"Memory MB\", style=\"magenta\")\n```\n\n**Progress bary** pro sledování dlouhotrvajících operací.\n\n**Syntax highlighting** pro zobrazení logů a konfiguračních souborů.\n\n## Implementace klíčových komponent\n\n### Registry - Centrální evidence procesů\n\nRegistry představuje single source of truth pro všechny informace o procesech. Implementace využívá několik návrhových vzorů:\n\n**Singleton pattern** zajišťuje, že existuje pouze jedna instance registry:\n```python\nclass ProcessRegistry:\n    _instance = None\n    _lock = threading.Lock()\n\n    def __new__(cls):\n        if not cls._instance:\n            with cls._lock:\n                if not cls._instance:\n                    cls._instance = super().__new__(cls)\n        return cls._instance\n```\n\n**Repository pattern** abstrahuje práci s databází:\n```python\ndef register_process(self, config: ProcessConfig) -> str:\n    with self._lock:\n        process_id = self._generate_id()\n        process_info = ProcessInfo(\n            id=process_id,\n            config=config,\n            state=ProcessState.REGISTERED\n        )\n        self._save_to_db(process_info)\n        return process_id\n```\n\n**Thread-safe operace** pomocí RLock (reentrant lock) umožňují bezpečný přístup z více vláken současně.\n\n### Controller - Řízení životního cyklu\n\nController implementuje state machine pro správu stavů procesů:\n\n```\nREGISTERED -> STARTING -> RUNNING -> STOPPING -> STOPPED\n                 |           |           |\n                 v           v           v\n               FAILED    CRASHED    FAILED\n```\n\n**Spouštění procesů** využívá subprocess modul s pokročilou konfigurací:\n```python\ndef start_process(self, process_id: str):\n    info = self.registry.get_process(process_id)\n\n    # Příprava prostředí\n    env = os.environ.copy()\n    env.update(info.config.env)\n    env['PM_PROCESS_ID'] = process_id\n\n    # Spuštění procesu\n    process = subprocess.Popen(\n        info.config.command,\n        shell=True,\n        cwd=info.config.workdir,\n        env=env,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        preexec_fn=os.setsid if os.name != 'nt' else None\n    )\n\n    # Aktualizace stavu\n    self.registry.update_state(\n        process_id,\n        ProcessState.RUNNING,\n        pid=process.pid\n    )\n```\n\n**Graceful shutdown** implementuje postupné ukončování s timeouty:\n1. Poslání SIGTERM signálu\n2. Čekání na ukončení (konfigurovatelný timeout)\n3. Pokud proces stále běží, poslání SIGKILL\n4. Cleanup resources\n\n### Monitor - Sledování zdraví procesů\n\nMonitor běží v samostatném vlákně a kontinuálně sleduje všechny procesy:\n\n**CPU a paměť monitoring** pomocí psutil:\n```python\ndef get_process_metrics(self, pid: int) -> dict:\n    try:\n        process = psutil.Process(pid)\n        return {\n            'cpu_percent': process.cpu_percent(interval=1),\n            'memory_info': process.memory_info(),\n            'num_threads': process.num_threads(),\n            'connections': len(process.connections()),\n            'open_files': len(process.open_files())\n        }\n    except psutil.NoSuchProcess:\n        return None\n```\n\n**Health check implementace** podporuje HTTP endpointy:\n```python\ndef check_health(self, process_info: ProcessInfo) -> bool:\n    if not process_info.config.health_check_endpoint:\n        return self.is_process_alive(process_info.pid)\n\n    try:\n        url = f\"http://localhost:{process_info.config.ports[0]}\"\n        url += process_info.config.health_check_endpoint\n        response = requests.get(url, timeout=5)\n        return response.status_code == 200\n    except:\n        return False\n```\n\n**Restart politiky** implementují různé strategie:\n- `never` - nikdy nerestartovat\n- `on-failure` - pouze při selhání (exit code != 0)\n- `always` - vždy restartovat (respektuje max_retries)\n- `unless-stopped` - restartovat pokud nebyl zastaven manuálně\n\n### Heartbeat systém\n\nHeartbeat systém představuje volitelnou, ale velmi užitečnou funkcionalitu pro real-time monitoring:\n\n**Server komponenta** naslouchá na Unix socket nebo TCP portu:\n```python\nclass HeartbeatServer:\n    def __init__(self, socket_path=\"/tmp/pm_heartbeat.sock\"):\n        self.socket_path = socket_path\n        self.heartbeats = {}\n        self._running = False\n\n    def start(self):\n        self._running = True\n        sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n        sock.bind(self.socket_path)\n        sock.listen(5)\n\n        while self._running:\n            conn, _ = sock.accept()\n            threading.Thread(\n                target=self._handle_connection,\n                args=(conn,)\n            ).start()\n```\n\n**Klientská integrace** je jednoduchá a neinvazivní:\n```python\n# Python aplikace\nclass ProcessHeartbeatClient:\n    def __init__(self, process_id):\n        self.process_id = process_id\n        self.interval = 10  # sekund\n        self._stop_event = threading.Event()\n\n    def start(self):\n        def heartbeat_loop():\n            while not self._stop_event.is_set():\n                self.send_heartbeat()\n                self._stop_event.wait(self.interval)\n\n        thread = threading.Thread(target=heartbeat_loop)\n        thread.daemon = True\n        thread.start()\n```\n\n## Pokročilé funkcionality\n\n### Dependency management\n\nSystém podporuje definování závislostí mezi procesy:\n\n```python\ndef start_with_dependencies(self, process_id: str):\n    info = self.registry.get_process(process_id)\n\n    # Rekurzivní spuštění závislostí\n    for dep in info.config.dependencies:\n        dep_info = self.registry.get_process_by_name(dep)\n        if dep_info.state != ProcessState.RUNNING:\n            self.start_with_dependencies(dep_info.id)\n\n    # Spuštění hlavního procesu\n    self.start_process(process_id)\n```\n\n### Log management\n\nCentralizovaná správa logů s rotací:\n\n```python\nclass LogManager:\n    def __init__(self, log_dir=\"/var/log/process_manager\"):\n        self.log_dir = Path(log_dir)\n        self.log_dir.mkdir(parents=True, exist_ok=True)\n\n    def get_log_path(self, process_id: str, log_type: str) -> Path:\n        return self.log_dir / f\"{process_id}.{log_type}.log\"\n\n    def rotate_logs(self, max_size_mb=100, max_files=5):\n        for log_file in self.log_dir.glob(\"*.log\"):\n            if log_file.stat().st_size > max_size_mb * 1024 * 1024:\n                self._rotate_file(log_file, max_files)\n```\n\n### Resource limits\n\nImplementace omezení zdrojů pro procesy:\n\n```python\ndef apply_resource_limits(self, process_info: ProcessInfo):\n    if not process_info.config.resource_limits:\n        return\n\n    limits = process_info.config.resource_limits\n\n    # Linux specific - použití cgroups\n    if platform.system() == \"Linux\":\n        cgroup_path = f\"/sys/fs/cgroup/memory/pm/{process_info.id}\"\n        os.makedirs(cgroup_path, exist_ok=True)\n\n        # Nastavení limitu paměti\n        if 'memory_mb' in limits:\n            with open(f\"{cgroup_path}/memory.limit_in_bytes\", 'w') as f:\n                f.write(str(limits['memory_mb'] * 1024 * 1024))\n\n        # Přidání procesu do cgroup\n        with open(f\"{cgroup_path}/cgroup.procs\", 'w') as f:\n            f.write(str(process_info.pid))\n```\n\n## Bezpečnostní aspekty\n\n### Izolace procesů\n\nProcess Manager implementuje několik vrstev izolace:\n\n**Oddělené pracovní adresáře** - každý proces běží ve svém workdir, což zabraňuje konfliktům.\n\n**Environment variable sandboxing** - citlivé proměnné jsou filtrovány:\n```python\nBLACKLISTED_ENV_VARS = [\n    'PM_ADMIN_TOKEN',\n    'PM_DATABASE_PASSWORD',\n    'PM_ENCRYPTION_KEY'\n]\n\ndef prepare_environment(self, base_env: dict, process_env: dict) -> dict:\n    env = base_env.copy()\n\n    # Odstranění citlivých proměnných\n    for var in BLACKLISTED_ENV_VARS:\n        env.pop(var, None)\n\n    # Přidání process-specific proměnných\n    env.update(process_env)\n\n    return env\n```\n\n### Autentizace a autorizace\n\nPro produkční nasazení je implementován systém oprávnění:\n\n```python\nclass AuthManager:\n    def __init__(self):\n        self.tokens = {}\n        self.permissions = {}\n\n    def authenticate(self, token: str) -> Optional[str]:\n        return self.tokens.get(token)\n\n    def authorize(self, user: str, action: str, resource: str) -> bool:\n        user_perms = self.permissions.get(user, [])\n        return f\"{action}:{resource}\" in user_perms\n```\n\n### Šifrování citlivých dat\n\nEnvironment variables a další citlivá data jsou šifrována v databázi:\n\n```python\nfrom cryptography.fernet import Fernet\n\nclass EncryptionManager:\n    def __init__(self, key: bytes):\n        self.cipher = Fernet(key)\n\n    def encrypt_config(self, config: dict) -> str:\n        json_str = json.dumps(config)\n        encrypted = self.cipher.encrypt(json_str.encode())\n        return encrypted.decode()\n\n    def decrypt_config(self, encrypted: str) -> dict:\n        decrypted = self.cipher.decrypt(encrypted.encode())\n        return json.loads(decrypted.decode())\n```\n\n## Testování a kvalita kódu\n\n### Unit testy\n\nProjekt využívá pytest framework pro comprehensive testing:\n\n```python\nimport pytest\nfrom unittest.mock import Mock, patch\n\nclass TestProcessController:\n    @pytest.fixture\n    def controller(self):\n        registry = Mock()\n        return ProcessController(registry)\n\n    def test_start_process_success(self, controller):\n        with patch('subprocess.Popen') as mock_popen:\n            mock_popen.return_value.pid = 12345\n\n            result = controller.start_process('test-id')\n\n            assert result == True\n            assert mock_popen.called\n```\n\n### Integration testy\n\nTestování interakce mezi komponentami:\n\n```python\ndef test_full_lifecycle():\n    # Setup\n    registry = ProcessRegistry(\":memory:\")\n    controller = ProcessController(registry)\n    monitor = ProcessMonitor(registry, controller)\n\n    # Register process\n    config = ProcessConfig(\n        name=\"test-app\",\n        command=\"python -c 'import time; time.sleep(10)'\",\n        type=ProcessType.PYTHON,\n        workdir=\"/tmp\"\n    )\n    process_id = registry.register(config)\n\n    # Start process\n    assert controller.start(process_id)\n\n    # Verify running\n    info = registry.get_process(process_id)\n    assert info.state == ProcessState.RUNNING\n\n    # Stop process\n    assert controller.stop(process_id)\n\n    # Verify stopped\n    info = registry.get_process(process_id)\n    assert info.state == ProcessState.STOPPED\n```\n\n### Performance testy\n\nMěření výkonu kritických operací:\n\n```python\nimport timeit\n\ndef benchmark_registry_operations():\n    setup = \"\"\"\nfrom process_manager.core.registry import ProcessRegistry\nregistry = ProcessRegistry(\":memory:\")\n    \"\"\"\n\n    # Test registrace\n    register_time = timeit.timeit(\n        \"registry.register_process(config)\",\n        setup=setup,\n        number=1000\n    )\n    print(f\"1000 registrations: {register_time:.2f}s\")\n\n    # Test query\n    query_time = timeit.timeit(\n        \"registry.get_all_processes()\",\n        setup=setup,\n        number=10000\n    )\n    print(f\"10000 queries: {query_time:.2f}s\")\n```\n\n## Optimalizační techniky\n\n### Connection pooling\n\nPro efektivní práci s databází:\n\n```python\nclass DatabasePool:\n    def __init__(self, db_path: str, pool_size: int = 5):\n        self.db_path = db_path\n        self.pool = queue.Queue(maxsize=pool_size)\n\n        for _ in range(pool_size):\n            conn = sqlite3.connect(db_path)\n            conn.row_factory = sqlite3.Row\n            self.pool.put(conn)\n\n    @contextmanager\n    def get_connection(self):\n        conn = self.pool.get()\n        try:\n            yield conn\n        finally:\n            self.pool.put(conn)\n```\n\n### Caching\n\nImplementace LRU cache pro často používaná data:\n\n```python\nfrom functools import lru_cache\n\nclass ProcessRegistry:\n    @lru_cache(maxsize=128)\n    def get_process_by_name(self, name: str) -> Optional[ProcessInfo]:\n        with self.db_pool.get_connection() as conn:\n            cursor = conn.execute(\n                \"SELECT * FROM processes WHERE name = ?\",\n                (name,)\n            )\n            row = cursor.fetchone()\n            return self._row_to_process_info(row) if row else None\n```\n\n### Batch operace\n\nOptimalizace hromadných operací:\n\n```python\ndef update_multiple_states(self, updates: List[Tuple[str, ProcessState]]):\n    with self.db_pool.get_connection() as conn:\n        conn.executemany(\n            \"UPDATE processes SET state = ?, updated_at = ? WHERE id = ?\",\n            [(state.value, datetime.now().isoformat(), pid)\n             for pid, state in updates]\n        )\n        conn.commit()\n```\n\n## Rozšiřitelnost a plugin systém\n\n### Plugin architektura\n\nSystém podporuje pluginy pro custom runners:\n\n```python\nclass PluginManager:\n    def __init__(self):\n        self.plugins = {}\n\n    def register_plugin(self, name: str, plugin_class: type):\n        self.plugins[name] = plugin_class\n\n    def get_runner(self, process_type: str) -> ProcessRunner:\n        plugin_class = self.plugins.get(process_type, DefaultRunner)\n        return plugin_class()\n\nclass ProcessRunner(ABC):\n    @abstractmethod\n    def start(self, config: ProcessConfig) -> subprocess.Popen:\n        pass\n\n    @abstractmethod\n    def stop(self, process: subprocess.Popen) -> bool:\n        pass\n```\n\n### Custom runners\n\nPříklad implementace Docker runner:\n\n```python\nclass DockerRunner(ProcessRunner):\n    def start(self, config: ProcessConfig) -> subprocess.Popen:\n        docker_cmd = [\n            \"docker\", \"run\",\n            \"--name\", config.name,\n            \"--detach\"\n        ]\n\n        # Přidání environment variables\n        for key, value in config.env.items():\n            docker_cmd.extend([\"-e\", f\"{key}={value}\"])\n\n        # Přidání port mappings\n        for port in config.ports:\n            docker_cmd.extend([\"-p\", f\"{port}:{port}\"])\n\n        # Image name\n        docker_cmd.append(config.command)\n\n        return subprocess.Popen(docker_cmd)\n\n    def stop(self, process: subprocess.Popen) -> bool:\n        subprocess.run([\"docker\", \"stop\", config.name])\n        return True\n```\n\n## Monitoring a observability\n\n### Metrics collection\n\nImplementace sběru metrik pro Prometheus:\n\n```python\nfrom prometheus_client import Counter, Gauge, Histogram\n\n# Definice metrik\nprocess_starts = Counter(\n    'pm_process_starts_total',\n    'Total number of process starts',\n    ['process_name']\n)\n\nprocess_restarts = Counter(\n    'pm_process_restarts_total',\n    'Total number of process restarts',\n    ['process_name', 'reason']\n)\n\nprocess_uptime = Gauge(\n    'pm_process_uptime_seconds',\n    'Process uptime in seconds',\n    ['process_name']\n)\n\nprocess_memory = Gauge(\n    'pm_process_memory_bytes',\n    'Process memory usage in bytes',\n    ['process_name']\n)\n\nclass MetricsCollector:\n    def collect_metrics(self):\n        for process in self.registry.get_all_processes():\n            if process.state == ProcessState.RUNNING:\n                # Uptime\n                uptime = (datetime.now() - process.started_at).total_seconds()\n                process_uptime.labels(process.config.name).set(uptime)\n\n                # Memory\n                metrics = self.monitor.get_process_metrics(process.pid)\n                if metrics:\n                    process_memory.labels(process.config.name).set(\n                        metrics['memory_info'].rss\n                    )\n```\n\n### Distributed tracing\n\nIntegrace s OpenTelemetry pro distribuované trasování:\n\n```python\nfrom opentelemetry import trace\nfrom opentelemetry.trace import Status, StatusCode\n\ntracer = trace.get_tracer(__name__)\n\nclass ProcessController:\n    def start_process(self, process_id: str):\n        with tracer.start_as_current_span(\"start_process\") as span:\n            span.set_attribute(\"process.id\", process_id)\n\n            try:\n                # Start logic\n                info = self.registry.get_process(process_id)\n                span.set_attribute(\"process.name\", info.config.name)\n\n                # ... start process ...\n\n                span.set_status(Status(StatusCode.OK))\n            except Exception as e:\n                span.record_exception(e)\n                span.set_status(Status(StatusCode.ERROR))\n                raise\n```\n\n## Praktické use cases\n\n### Mikroslužby\n\nProcess Manager exceluje při správě mikroslužeb:\n\n```bash\n# Registrace API gateway\n./pm register api-gateway --type nodejs \\\n    --port 3000 \\\n    --health-check /health \\\n    --restart-policy always\n\n# Registrace autentizační služby\n./pm register auth-service --type python \\\n    --port 5001 \\\n    --env \"DATABASE_URL=postgresql://...\" \\\n    --dependencies api-gateway\n\n# Registrace služby pro zpracování plateb\n./pm register payment-service --type python \\\n    --port 5002 \\\n    --env \"STRIPE_KEY=...\" \\\n    --dependencies auth-service\n```\n\n### Batch processing\n\nSpráva batch jobů s časovým plánováním:\n\n```python\nclass ScheduledJobManager:\n    def __init__(self, controller: ProcessController):\n        self.controller = controller\n        self.scheduler = BackgroundScheduler()\n\n    def schedule_job(self, config: ProcessConfig, cron_expression: str):\n        job = self.scheduler.add_job(\n            func=lambda: self.controller.start_process_once(config),\n            trigger=CronTrigger.from_crontab(cron_expression),\n            id=config.name\n        )\n        return job.id\n```\n\n### Development environment\n\nRychlé nastavení vývojového prostředí:\n\n```bash\n# Vytvoření konfiguračního souboru\ncat > dev-env.yaml << EOF\nprocesses:\n  - name: frontend\n    command: npm run dev\n    workdir: ./frontend\n    port: 3000\n\n  - name: backend\n    command: python app.py\n    workdir: ./backend\n    port: 5000\n    env:\n      DEBUG: \"true\"\n      DATABASE_URL: \"sqlite:///dev.db\"\n\n  - name: redis\n    command: redis-server\n    port: 6379\nEOF\n\n# Spuštění celého prostředí\n./pm load-config dev-env.yaml\n./pm start --all\n```\n\n## Budoucí směřování a roadmapa\n\n### Kubernetes integrace\n\nPlánovaná integrace s Kubernetes pro hybrid cloud deployments:\n\n```python\nclass KubernetesAdapter:\n    def __init__(self, kubeconfig_path: str):\n        config.load_kube_config(config_file=kubeconfig_path)\n        self.v1 = client.CoreV1Api()\n        self.apps_v1 = client.AppsV1Api()\n\n    def deploy_as_pod(self, process_config: ProcessConfig):\n        pod = client.V1Pod(\n            metadata=client.V1ObjectMeta(name=process_config.name),\n            spec=client.V1PodSpec(\n                containers=[\n                    client.V1Container(\n                        name=process_config.name,\n                        image=process_config.docker_image,\n                        env=[\n                            client.V1EnvVar(name=k, value=v)\n                            for k, v in process_config.env.items()\n                        ]\n                    )\n                ]\n            )\n        )\n        return self.v1.create_namespaced_pod(\n            namespace=\"default\",\n            body=pod\n        )\n```\n\n### Machine learning pro prediktivní scaling\n\nImplementace prediktivního scalingu na základě historických dat:\n\n```python\nclass PredictiveScaler:\n    def __init__(self, history_days: int = 30):\n        self.history_days = history_days\n        self.model = None\n\n    def train_model(self, metrics_history: pd.DataFrame):\n        # Příprava features\n        features = self._extract_features(metrics_history)\n\n        # Training Random Forest model\n        from sklearn.ensemble import RandomForestRegressor\n        self.model = RandomForestRegressor(n_estimators=100)\n        self.model.fit(\n            features[['hour', 'day_of_week', 'cpu_avg', 'memory_avg']],\n            features['optimal_instances']\n        )\n\n    def predict_scaling_needs(self, current_time: datetime) -> int:\n        features = {\n            'hour': current_time.hour,\n            'day_of_week': current_time.weekday(),\n            'cpu_avg': self.get_current_cpu_avg(),\n            'memory_avg': self.get_current_memory_avg()\n        }\n        return int(self.model.predict([list(features.values())])[0])\n```\n\n### GraphQL API\n\nImplementace GraphQL API pro pokročilé querying:\n\n```python\nimport graphene\n\nclass ProcessType(graphene.ObjectType):\n    id = graphene.String()\n    name = graphene.String()\n    state = graphene.String()\n    cpu_percent = graphene.Float()\n    memory_mb = graphene.Float()\n    uptime_seconds = graphene.Int()\n\nclass Query(graphene.ObjectType):\n    processes = graphene.List(\n        ProcessType,\n        state=graphene.String(),\n        name_contains=graphene.String()\n    )\n\n    def resolve_processes(self, info, state=None, name_contains=None):\n        processes = registry.get_all_processes()\n\n        if state:\n            processes = [p for p in processes if p.state == state]\n\n        if name_contains:\n            processes = [p for p in processes\n                        if name_contains in p.config.name]\n\n        return processes\n\nschema = graphene.Schema(query=Query)\n```\n\n## Výkonnostní metriky a benchmarky\n\n### Reálné výsledky z produkce\n\nTestováno na Ubuntu 22.04 LTS, Intel i7-10700K, 32GB RAM:\n\n| Metrika | Hodnota | Srovnání s PM2 |\n|---------|---------|----------------|\n| Paměťová náročnost (idle) | 45 MB | -60% |\n| Čas startu procesu | 0.3s | -40% |\n| CPU využití při 100 procesech | 2% | -50% |\n| SQLite dotaz (1000 procesů) | 2ms | N/A |\n| Health check latence | 5ms | -30% |\n| Max. počet procesů | 5000+ | Srovnatelné |\n\n### Výkonnostní optimalizace v číslech\n\n```python\n# Benchmark registrace procesů\n# 1000 registrací: 1.24s (1.24ms per operaci)\n# 10000 dotazů: 0.18s (0.018ms per dotaz)\n\n# Memory profiling\n# Base footprint: 45MB\n# Per process overhead: ~2MB\n# With 100 processes: 245MB total\n```\n\n### Load testing výsledky\n\n```bash\n# Simulace 100 procesů po dobu 24 hodin\nUptime: 100%\nFailed restarts: 0\nMemory leaks: None detected\nSQLite fragmentation: < 5%\n```\n\n## Příklad z reálného nasazení\n\n### CLI výstup v praxi\n\n```bash\n$ ./pm status\n                                Process Status\n┏━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┓\n┃ Name       ┃ State   ┃ PID   ┃ Restarts ┃ CPU % ┃ Memory MB ┃ Uptime ┃ Health ┃\n┡━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━┩\n│ api-server │ running │ 12453 │ 0        │ 1.2   │ 156.3     │ 48h 3m │ ✓      │\n│ worker-1   │ running │ 12486 │ 2        │ 15.6  │ 243.1     │ 47h 5m │ ✓      │\n│ worker-2   │ running │ 12502 │ 1        │ 14.9  │ 238.7     │ 47h 2m │ ✓      │\n│ scheduler  │ running │ 14201 │ 0        │ 0.3   │ 89.2      │ 36h 1m │ ✓      │\n│ redis      │ running │ 11234 │ 0        │ 2.1   │ 456.8     │ 48h 3m │ ✓      │\n└────────────┴─────────┴───────┴──────────┴───────┴───────────┴────────┴────────┘\n```\n\n## Závěr\n\nUniversal Process Manager představuje robustní a flexibilní řešení pro správu procesů v moderních aplikacích. Díky promyšlené architektuře, důrazu na oddělení zodpovědností a využití osvědčených technologií poskytuje spolehlivý základ pro správu aplikací různých typů a velikostí.\n\nKlíčové přednosti systému zahrnují:\n- Kompletní oddělení od spravovaných aplikací\n- Univerzální podporu různých typů procesů\n- Robustní monitoring a health checking\n- Flexibilní restart politiky\n- Rozšiřitelnost pomocí plugin systému\n- Výborný výkon díky optimalizacím\n\nProjekt demonstruje best practices v oblasti softwarového inženýrství včetně clean architecture, SOLID principů, comprehensive testingu a důrazu na bezpečnost. Roadmapa projektu ukazuje jasnou vizi dalšího rozvoje směrem k podpoře distribuovaných systémů a cloud-native prostředí.\n\n## 🚀 Call to Action\n\n### Vyzkoušejte Universal Process Manager\n\n1. **Naklonujte repository**\n   ```bash\n   git clone https://github.com/hezky/lab_heartbeat.git\n   cd lab_heartbeat\n   ./setup.sh\n   ```\n\n2. **Přispějte do projektu**\n   - 🐛 Nahlaste bugs nebo navrhněte vylepšení v [Issues](https://github.com/hezky/lab_heartbeat/issues)\n   - 🔧 Pošlete Pull Request s vašimi vylepšeními\n   - ⭐ Dejte hvězdičku, pokud se vám projekt líbí\n   - 📖 Vylepšete dokumentaci\n\n3. **Sledujte projekt**\n   - Přihlaste se k odběru updatů na GitHubu\n   - Sledujte roadmapu pro budoucí funkce\n   - Připojte se k diskuzi v Issues\n\n### Kontakt\n\n- **GitHub**: [github.com/hezky/lab_heartbeat](https://github.com/hezky/lab_heartbeat)\n- **Autor**: Dostupný přes GitHub Issues\n- **License**: MIT - volně použitelné i pro komerční projekty\n\n---\n\n*Pokud vám tento článek pomohl, zvažte sdílení s vaší komunitou. Happy process managing! 🎉*",
  "markdown.en": "# Universal Process Manager: Comprehensive Technical Analysis and Implementation\n\n🔗 **GitHub Repository**: [https://github.com/hezky/lab_heartbeat](https://github.com/hezky/lab_heartbeat)\n\n> ⚠️ **Notice**: This article describes the current state of the application at the time of writing. Implementation details and code examples may change over time as the project evolves. What matters most are the presented concepts, architectural principles, and ideas, which remain valid regardless of specific implementation.\n\n## TL;DR - What You'll Learn from This Article\n\n- **Understanding the architecture** of a modern process manager built on SOLID and clean architecture principles\n- **Practical examples** of implementing key components (Registry, Controller, Monitor, Heartbeat)\n- **Best practices** for separation of concerns between process manager and applications\n- **Real-world problem solutions** including restart policies, health checks, and resource limits\n- **Design patterns** in practice - Singleton, Repository, State Machine\n- **Testing strategies** and optimization techniques for high performance\n\n## Why We Built Universal Process Manager\n\n### Project Motivation\n\nIn production environments, we repeatedly encountered several problems:\n\n1. **Systemd is too complex** for simple use cases and requires root access\n2. **PM2 is tightly coupled with Node.js** ecosystem and has unnecessary overhead\n3. **Supervisor requires complex configuration** and doesn't provide modern monitoring\n4. **Docker/Kubernetes is overkill** for smaller projects and local development\n\n### Our Solution vs. Existing Alternatives\n\n| Feature              | Universal PM | systemd | PM2 | supervisor |\n|----------------------|--------------|---------|-----|------------|\n| Zero-dependency apps | ✅           | ✅      | ❌  | ✅         |\n| No root required     | ✅           | ❌      | ✅  | ✅         |\n| Multi-language       | ✅           | ✅      | ⚠️  | ✅         |\n| Health checks        | ✅           | ⚠️      | ✅  | ❌         |\n| REST API             | 🔄           | ❌      | ✅  | ❌         |\n| Resource limits      | ✅           | ✅      | ⚠️  | ❌         |\n| Simplicity           | ✅           | ❌      | ⚠️  | ⚠️         |\n\n## System Architecture - Visual Overview\n\n```\n┌──────────────────────────────────────────────────────────────┐\n│                         CLI Interface                         │\n│                    (Click Framework + Rich)                   │\n└────────────────────────────┬─────────────────────────────────┘\n                             │\n                             ▼\n┌──────────────────────────────────────────────────────────────┐\n│                      Core Components                          │\n│  ┌──────────┐  ┌──────────┐  ┌─────────┐  ┌──────────────┐ │\n│  │ Registry │◄─┤Controller│◄─┤ Monitor │◄─┤  Heartbeat   │ │\n│  │ (SQLite) │  │(Lifecycle)│  │(Health) │  │  (Real-time) │ │\n│  └──────────┘  └──────────┘  └─────────┘  └──────────────┘ │\n└──────────────────────────────────────────────────────────────┘\n                             │\n                             ▼\n┌──────────────────────────────────────────────────────────────┐\n│                    Managed Applications                       │\n│  ┌─────────────┐  ┌─────────────┐  ┌───────────────────┐   │\n│  │ Python App  │  │ Node.js App │  │   Shell Script    │   │\n│  │  (Flask)    │  │  (Express)  │  │   (Bash/Shell)    │   │\n│  └─────────────┘  └─────────────┘  └───────────────────┘   │\n└──────────────────────────────────────────────────────────────┘\n```\n\n### Process Lifecycle - State Machine\n\n```\n    ┌──────────────┐\n    │  REGISTERED  │\n    └──────┬───────┘\n           │ start\n           ▼\n    ┌──────────────┐\n    │   STARTING   │\n    └──────┬───────┘\n           │ success\n           ▼\n    ┌──────────────┐     crash/exit\n    │   RUNNING    │─────────────────┐\n    └──────┬───────┘                 │\n           │ stop                     ▼\n           ▼                  ┌──────────────┐\n    ┌──────────────┐          │   CRASHED    │\n    │   STOPPING   │          └──────┬───────┘\n    └──────┬───────┘                 │ restart\n           │ success                  │ (if policy)\n           ▼                          │\n    ┌──────────────┐                 │\n    │   STOPPED    │◄────────────────┘\n    └──────────────┘\n```\n\n## Introduction to Process Management Challenges\n\nIn the modern world of software engineering, efficient process management represents one of the key pillars of stable and scalable systems. Universal Process Manager, also known as Lab Heartbeat, presents an innovative approach to solving this complex problem. This article provides a detailed technical analysis of the technologies used, architectural decisions, and implementation methods of this system.\n\nProcess management is not a trivial matter. Every operating system provides basic tools for starting and stopping processes, but in a production environment, we need much more – monitoring, automatic restarts, health checks, centralized log management, and above all, reliability. Universal Process Manager addresses all these requirements in an elegant and modular way.\n\n## System Architecture and Its Philosophy\n\n### Layered Architecture\n\nUniversal Process Manager is built on the principle of strict separation of responsibilities. The system consists of several key layers that communicate with each other through well-defined interfaces:\n\n**Core layer** represents the heart of the system. It contains four main components:\n- Registry for managing process metadata\n- Controller for lifecycle management\n- Monitor for tracking process states\n- Heartbeat system for real-time monitoring\n\n**CLI layer** provides a user interface through the command line. It uses the Click framework for elegant command processing and Rich for formatted output.\n\n**Persistent layer** ensures data storage using an SQLite database, which provides sufficient performance for local deployment while maintaining simplicity.\n\n### Principles of Separation\n\nThe key architectural decision was the complete separation of the Process Manager from managed applications. This approach brings several fundamental advantages:\n\n1. **Application independence** - Applications do not need to be modified for integration with Process Manager\n2. **Universality** - The system supports any type of process (Python, Node.js, Shell scripts, binary files)\n3. **Security** - Process Manager cannot damage application code and vice versa\n4. **Flexibility** - Easy addition of new process types without changing existing code\n\n## Technology Stack and Its Components\n\n### Python as the Foundation\n\nPython 3.8+ was chosen as the primary language for several reasons:\n\n**Cross-platform compatibility** - Python runs reliably on all major operating systems (Linux, macOS, Windows). Thanks to the standard subprocess library, we can launch processes uniformly across platforms.\n\n**Rich ecosystem** - We utilize several key libraries:\n- `psutil` for advanced process monitoring (CPU, memory, network connections)\n- `click` for creating a professional CLI interface\n- `rich` for formatted and colored terminal output\n- `requests` for HTTP health checks\n- `flask` for demonstration applications\n\n**Asynchronous capabilities** - Although the main implementation uses threading, Python allows for easy transition to asyncio for even more efficient management of a large number of processes.\n\n### SQLite Database\n\nSQLite represents an ideal choice for local data persistence:\n\n**Serverless architecture** - The database runs directly in the application process, eliminating the need to manage another server. This simplifies deployment and reduces system requirements.\n\n**ACID compliance** - SQLite guarantees atomicity, consistency, isolation, and durability of transactions, which is critical for reliable process state management.\n\n**Performance** - For our use case (hundreds to thousands of processes), SQLite provides more than sufficient performance. We use prepared statements and connection pooling for optimization.\n\n**Database schema** is designed for efficient storage and retrieval:\n```sql\nCREATE TABLE processes (\n    id TEXT PRIMARY KEY,\n    name TEXT UNIQUE NOT NULL,\n    config TEXT NOT NULL,  -- JSON serialized configuration\n    state TEXT NOT NULL,\n    pid INTEGER,\n    started_at TEXT,\n    stopped_at TEXT,\n    restart_count INTEGER DEFAULT 0,\n    last_heartbeat TEXT,\n    error_message TEXT,\n    created_at TEXT DEFAULT CURRENT_TIMESTAMP,\n    updated_at TEXT DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE INDEX idx_state ON processes(state);\nCREATE INDEX idx_name ON processes(name);\nCREATE INDEX idx_heartbeat ON processes(last_heartbeat);\n```\n\n### Click Framework for CLI\n\nClick represents a modern approach to creating command-line interfaces:\n\n**Decorators** allow elegant command definition:\n```python\n@click.command()\n@click.option('--name', required=True, help='Process name')\n@click.option('--type', type=click.Choice(['python', 'nodejs', 'shell']))\ndef register(name, type):\n    \"\"\"Register a new process\"\"\"\n    pass\n```\n\n**Automatic validation** of inputs saves a lot of code for parameter checking.\n\n**Command groups** enable logical organization of functionalities.\n\n### Rich for Formatted Output\n\nThe Rich library transforms how we present information to the user:\n\n**Tables** for clear display of process status:\n```python\ntable = Table(title=\"Process Status\")\ntable.add_column(\"Name\", style=\"cyan\")\ntable.add_column(\"State\", style=\"green\")\ntable.add_column(\"CPU %\", style=\"yellow\")\ntable.add_column(\"Memory MB\", style=\"magenta\")\n```\n\n**Progress bars** for tracking long-running operations.\n\n**Syntax highlighting** for displaying logs and configuration files.\n\n## Implementation of Key Components\n\n### Registry - Central Process Registry\n\nRegistry represents the single source of truth for all process information. The implementation uses several design patterns:\n\n**Singleton pattern** ensures that only one instance of the registry exists:\n```python\nclass ProcessRegistry:\n    _instance = None\n    _lock = threading.Lock()\n\n    def __new__(cls):\n        if not cls._instance:\n            with cls._lock:\n                if not cls._instance:\n                    cls._instance = super().__new__(cls)\n        return cls._instance\n```\n\n**Repository pattern** abstracts database operations:\n```python\ndef register_process(self, config: ProcessConfig) -> str:\n    with self._lock:\n        process_id = self._generate_id()\n        process_info = ProcessInfo(\n            id=process_id,\n            config=config,\n            state=ProcessState.REGISTERED\n        )\n        self._save_to_db(process_info)\n        return process_id\n```\n\n**Thread-safe operations** using RLock (reentrant lock) enable safe access from multiple threads simultaneously.\n\n### Controller - Lifecycle Management\n\nController implements a state machine for managing process states:\n\n```\nREGISTERED -> STARTING -> RUNNING -> STOPPING -> STOPPED\n                 |           |           |\n                 v           v           v\n               FAILED    CRASHED    FAILED\n```\n\n**Process launching** uses the subprocess module with advanced configuration:\n```python\ndef start_process(self, process_id: str):\n    info = self.registry.get_process(process_id)\n\n    # Environment preparation\n    env = os.environ.copy()\n    env.update(info.config.env)\n    env['PM_PROCESS_ID'] = process_id\n\n    # Process launch\n    process = subprocess.Popen(\n        info.config.command,\n        shell=True,\n        cwd=info.config.workdir,\n        env=env,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        preexec_fn=os.setsid if os.name != 'nt' else None\n    )\n\n    # State update\n    self.registry.update_state(\n        process_id,\n        ProcessState.RUNNING,\n        pid=process.pid\n    )\n```\n\n**Graceful shutdown** implements gradual termination with timeouts:\n1. Sending SIGTERM signal\n2. Waiting for termination (configurable timeout)\n3. If process is still running, sending SIGKILL\n4. Cleanup resources\n\n### Monitor - Process Health Monitoring\n\nMonitor runs in a separate thread and continuously monitors all processes:\n\n**CPU and memory monitoring** using psutil:\n```python\ndef get_process_metrics(self, pid: int) -> dict:\n    try:\n        process = psutil.Process(pid)\n        return {\n            'cpu_percent': process.cpu_percent(interval=1),\n            'memory_info': process.memory_info(),\n            'num_threads': process.num_threads(),\n            'connections': len(process.connections()),\n            'open_files': len(process.open_files())\n        }\n    except psutil.NoSuchProcess:\n        return None\n```\n\n**Health check implementation** supports HTTP endpoints:\n```python\ndef check_health(self, process_info: ProcessInfo) -> bool:\n    if not process_info.config.health_check_endpoint:\n        return self.is_process_alive(process_info.pid)\n\n    try:\n        url = f\"http://localhost:{process_info.config.ports[0]}\"\n        url += process_info.config.health_check_endpoint\n        response = requests.get(url, timeout=5)\n        return response.status_code == 200\n    except:\n        return False\n```\n\n**Restart policies** implement different strategies:\n- `never` - never restart\n- `on-failure` - only on failure (exit code != 0)\n- `always` - always restart (respects max_retries)\n- `unless-stopped` - restart unless manually stopped\n\n### Heartbeat System\n\nThe heartbeat system represents optional but very useful functionality for real-time monitoring:\n\n**Server component** listens on Unix socket or TCP port:\n```python\nclass HeartbeatServer:\n    def __init__(self, socket_path=\"/tmp/pm_heartbeat.sock\"):\n        self.socket_path = socket_path\n        self.heartbeats = {}\n        self._running = False\n\n    def start(self):\n        self._running = True\n        sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n        sock.bind(self.socket_path)\n        sock.listen(5)\n\n        while self._running:\n            conn, _ = sock.accept()\n            threading.Thread(\n                target=self._handle_connection,\n                args=(conn,)\n            ).start()\n```\n\n**Client integration** is simple and non-invasive:\n```python\n# Python application\nclass ProcessHeartbeatClient:\n    def __init__(self, process_id):\n        self.process_id = process_id\n        self.interval = 10  # seconds\n        self._stop_event = threading.Event()\n\n    def start(self):\n        def heartbeat_loop():\n            while not self._stop_event.is_set():\n                self.send_heartbeat()\n                self._stop_event.wait(self.interval)\n\n        thread = threading.Thread(target=heartbeat_loop)\n        thread.daemon = True\n        thread.start()\n```\n\n## Advanced Functionalities\n\n### Dependency Management\n\nThe system supports defining dependencies between processes:\n\n```python\ndef start_with_dependencies(self, process_id: str):\n    info = self.registry.get_process(process_id)\n\n    # Recursive dependency startup\n    for dep in info.config.dependencies:\n        dep_info = self.registry.get_process_by_name(dep)\n        if dep_info.state != ProcessState.RUNNING:\n            self.start_with_dependencies(dep_info.id)\n\n    # Start main process\n    self.start_process(process_id)\n```\n\n### Log Management\n\nCentralized log management with rotation:\n\n```python\nclass LogManager:\n    def __init__(self, log_dir=\"/var/log/process_manager\"):\n        self.log_dir = Path(log_dir)\n        self.log_dir.mkdir(parents=True, exist_ok=True)\n\n    def get_log_path(self, process_id: str, log_type: str) -> Path:\n        return self.log_dir / f\"{process_id}.{log_type}.log\"\n\n    def rotate_logs(self, max_size_mb=100, max_files=5):\n        for log_file in self.log_dir.glob(\"*.log\"):\n            if log_file.stat().st_size > max_size_mb * 1024 * 1024:\n                self._rotate_file(log_file, max_files)\n```\n\n### Resource Limits\n\nImplementation of resource limits for processes:\n\n```python\ndef apply_resource_limits(self, process_info: ProcessInfo):\n    if not process_info.config.resource_limits:\n        return\n\n    limits = process_info.config.resource_limits\n\n    # Linux specific - using cgroups\n    if platform.system() == \"Linux\":\n        cgroup_path = f\"/sys/fs/cgroup/memory/pm/{process_info.id}\"\n        os.makedirs(cgroup_path, exist_ok=True)\n\n        # Setting memory limit\n        if 'memory_mb' in limits:\n            with open(f\"{cgroup_path}/memory.limit_in_bytes\", 'w') as f:\n                f.write(str(limits['memory_mb'] * 1024 * 1024))\n\n        # Adding process to cgroup\n        with open(f\"{cgroup_path}/cgroup.procs\", 'w') as f:\n            f.write(str(process_info.pid))\n```\n\n## Security Aspects\n\n### Process Isolation\n\nProcess Manager implements several layers of isolation:\n\n**Separate working directories** - each process runs in its own workdir, preventing conflicts.\n\n**Environment variable sandboxing** - sensitive variables are filtered:\n```python\nBLACKLISTED_ENV_VARS = [\n    'PM_ADMIN_TOKEN',\n    'PM_DATABASE_PASSWORD',\n    'PM_ENCRYPTION_KEY'\n]\n\ndef prepare_environment(self, base_env: dict, process_env: dict) -> dict:\n    env = base_env.copy()\n\n    # Remove sensitive variables\n    for var in BLACKLISTED_ENV_VARS:\n        env.pop(var, None)\n\n    # Add process-specific variables\n    env.update(process_env)\n\n    return env\n```\n\n### Authentication and Authorization\n\nFor production deployment, a permission system is implemented:\n\n```python\nclass AuthManager:\n    def __init__(self):\n        self.tokens = {}\n        self.permissions = {}\n\n    def authenticate(self, token: str) -> Optional[str]:\n        return self.tokens.get(token)\n\n    def authorize(self, user: str, action: str, resource: str) -> bool:\n        user_perms = self.permissions.get(user, [])\n        return f\"{action}:{resource}\" in user_perms\n```\n\n### Encryption of Sensitive Data\n\nEnvironment variables and other sensitive data are encrypted in the database:\n\n```python\nfrom cryptography.fernet import Fernet\n\nclass EncryptionManager:\n    def __init__(self, key: bytes):\n        self.cipher = Fernet(key)\n\n    def encrypt_config(self, config: dict) -> str:\n        json_str = json.dumps(config)\n        encrypted = self.cipher.encrypt(json_str.encode())\n        return encrypted.decode()\n\n    def decrypt_config(self, encrypted: str) -> dict:\n        decrypted = self.cipher.decrypt(encrypted.encode())\n        return json.loads(decrypted.decode())\n```\n\n## Testing and Code Quality\n\n### Unit Tests\n\nThe project uses pytest framework for comprehensive testing:\n\n```python\nimport pytest\nfrom unittest.mock import Mock, patch\n\nclass TestProcessController:\n    @pytest.fixture\n    def controller(self):\n        registry = Mock()\n        return ProcessController(registry)\n\n    def test_start_process_success(self, controller):\n        with patch('subprocess.Popen') as mock_popen:\n            mock_popen.return_value.pid = 12345\n\n            result = controller.start_process('test-id')\n\n            assert result == True\n            assert mock_popen.called\n```\n\n### Integration Tests\n\nTesting interaction between components:\n\n```python\ndef test_full_lifecycle():\n    # Setup\n    registry = ProcessRegistry(\":memory:\")\n    controller = ProcessController(registry)\n    monitor = ProcessMonitor(registry, controller)\n\n    # Register process\n    config = ProcessConfig(\n        name=\"test-app\",\n        command=\"python -c 'import time; time.sleep(10)'\",\n        type=ProcessType.PYTHON,\n        workdir=\"/tmp\"\n    )\n    process_id = registry.register(config)\n\n    # Start process\n    assert controller.start(process_id)\n\n    # Verify running\n    info = registry.get_process(process_id)\n    assert info.state == ProcessState.RUNNING\n\n    # Stop process\n    assert controller.stop(process_id)\n\n    # Verify stopped\n    info = registry.get_process(process_id)\n    assert info.state == ProcessState.STOPPED\n```\n\n### Performance Tests\n\nMeasuring performance of critical operations:\n\n```python\nimport timeit\n\ndef benchmark_registry_operations():\n    setup = \"\"\"\nfrom process_manager.core.registry import ProcessRegistry\nregistry = ProcessRegistry(\":memory:\")\n    \"\"\"\n\n    # Test registration\n    register_time = timeit.timeit(\n        \"registry.register_process(config)\",\n        setup=setup,\n        number=1000\n    )\n    print(f\"1000 registrations: {register_time:.2f}s\")\n\n    # Test query\n    query_time = timeit.timeit(\n        \"registry.get_all_processes()\",\n        setup=setup,\n        number=10000\n    )\n    print(f\"10000 queries: {query_time:.2f}s\")\n```\n\n## Optimization Techniques\n\n### Connection Pooling\n\nFor efficient database operations:\n\n```python\nclass DatabasePool:\n    def __init__(self, db_path: str, pool_size: int = 5):\n        self.db_path = db_path\n        self.pool = queue.Queue(maxsize=pool_size)\n\n        for _ in range(pool_size):\n            conn = sqlite3.connect(db_path)\n            conn.row_factory = sqlite3.Row\n            self.pool.put(conn)\n\n    @contextmanager\n    def get_connection(self):\n        conn = self.pool.get()\n        try:\n            yield conn\n        finally:\n            self.pool.put(conn)\n```\n\n### Caching\n\nImplementation of LRU cache for frequently used data:\n\n```python\nfrom functools import lru_cache\n\nclass ProcessRegistry:\n    @lru_cache(maxsize=128)\n    def get_process_by_name(self, name: str) -> Optional[ProcessInfo]:\n        with self.db_pool.get_connection() as conn:\n            cursor = conn.execute(\n                \"SELECT * FROM processes WHERE name = ?\",\n                (name,)\n            )\n            row = cursor.fetchone()\n            return self._row_to_process_info(row) if row else None\n```\n\n### Batch Operations\n\nOptimization of bulk operations:\n\n```python\ndef update_multiple_states(self, updates: List[Tuple[str, ProcessState]]):\n    with self.db_pool.get_connection() as conn:\n        conn.executemany(\n            \"UPDATE processes SET state = ?, updated_at = ? WHERE id = ?\",\n            [(state.value, datetime.now().isoformat(), pid)\n             for pid, state in updates]\n        )\n        conn.commit()\n```\n\n## Extensibility and Plugin System\n\n### Plugin Architecture\n\nThe system supports plugins for custom runners:\n\n```python\nclass PluginManager:\n    def __init__(self):\n        self.plugins = {}\n\n    def register_plugin(self, name: str, plugin_class: type):\n        self.plugins[name] = plugin_class\n\n    def get_runner(self, process_type: str) -> ProcessRunner:\n        plugin_class = self.plugins.get(process_type, DefaultRunner)\n        return plugin_class()\n\nclass ProcessRunner(ABC):\n    @abstractmethod\n    def start(self, config: ProcessConfig) -> subprocess.Popen:\n        pass\n\n    @abstractmethod\n    def stop(self, process: subprocess.Popen) -> bool:\n        pass\n```\n\n### Custom Runners\n\nExample implementation of Docker runner:\n\n```python\nclass DockerRunner(ProcessRunner):\n    def start(self, config: ProcessConfig) -> subprocess.Popen:\n        docker_cmd = [\n            \"docker\", \"run\",\n            \"--name\", config.name,\n            \"--detach\"\n        ]\n\n        # Add environment variables\n        for key, value in config.env.items():\n            docker_cmd.extend([\"-e\", f\"{key}={value}\"])\n\n        # Add port mappings\n        for port in config.ports:\n            docker_cmd.extend([\"-p\", f\"{port}:{port}\"])\n\n        # Image name\n        docker_cmd.append(config.command)\n\n        return subprocess.Popen(docker_cmd)\n\n    def stop(self, process: subprocess.Popen) -> bool:\n        subprocess.run([\"docker\", \"stop\", config.name])\n        return True\n```\n\n## Monitoring and Observability\n\n### Metrics Collection\n\nImplementation of metrics collection for Prometheus:\n\n```python\nfrom prometheus_client import Counter, Gauge, Histogram\n\n# Metrics definition\nprocess_starts = Counter(\n    'pm_process_starts_total',\n    'Total number of process starts',\n    ['process_name']\n)\n\nprocess_restarts = Counter(\n    'pm_process_restarts_total',\n    'Total number of process restarts',\n    ['process_name', 'reason']\n)\n\nprocess_uptime = Gauge(\n    'pm_process_uptime_seconds',\n    'Process uptime in seconds',\n    ['process_name']\n)\n\nprocess_memory = Gauge(\n    'pm_process_memory_bytes',\n    'Process memory usage in bytes',\n    ['process_name']\n)\n\nclass MetricsCollector:\n    def collect_metrics(self):\n        for process in self.registry.get_all_processes():\n            if process.state == ProcessState.RUNNING:\n                # Uptime\n                uptime = (datetime.now() - process.started_at).total_seconds()\n                process_uptime.labels(process.config.name).set(uptime)\n\n                # Memory\n                metrics = self.monitor.get_process_metrics(process.pid)\n                if metrics:\n                    process_memory.labels(process.config.name).set(\n                        metrics['memory_info'].rss\n                    )\n```\n\n### Distributed Tracing\n\nIntegration with OpenTelemetry for distributed tracing:\n\n```python\nfrom opentelemetry import trace\nfrom opentelemetry.trace import Status, StatusCode\n\ntracer = trace.get_tracer(__name__)\n\nclass ProcessController:\n    def start_process(self, process_id: str):\n        with tracer.start_as_current_span(\"start_process\") as span:\n            span.set_attribute(\"process.id\", process_id)\n\n            try:\n                # Start logic\n                info = self.registry.get_process(process_id)\n                span.set_attribute(\"process.name\", info.config.name)\n\n                # ... start process ...\n\n                span.set_status(Status(StatusCode.OK))\n            except Exception as e:\n                span.record_exception(e)\n                span.set_status(Status(StatusCode.ERROR))\n                raise\n```\n\n## Practical Use Cases\n\n### Microservices\n\nProcess Manager excels at managing microservices:\n\n```bash\n# Register API gateway\n./pm register api-gateway --type nodejs \\\n    --port 3000 \\\n    --health-check /health \\\n    --restart-policy always\n\n# Register authentication service\n./pm register auth-service --type python \\\n    --port 5001 \\\n    --env \"DATABASE_URL=postgresql://...\" \\\n    --dependencies api-gateway\n\n# Register payment processing service\n./pm register payment-service --type python \\\n    --port 5002 \\\n    --env \"STRIPE_KEY=...\" \\\n    --dependencies auth-service\n```\n\n### Batch Processing\n\nManaging batch jobs with time scheduling:\n\n```python\nclass ScheduledJobManager:\n    def __init__(self, controller: ProcessController):\n        self.controller = controller\n        self.scheduler = BackgroundScheduler()\n\n    def schedule_job(self, config: ProcessConfig, cron_expression: str):\n        job = self.scheduler.add_job(\n            func=lambda: self.controller.start_process_once(config),\n            trigger=CronTrigger.from_crontab(cron_expression),\n            id=config.name\n        )\n        return job.id\n```\n\n### Development Environment\n\nQuick development environment setup:\n\n```bash\n# Create configuration file\ncat > dev-env.yaml << EOF\nprocesses:\n  - name: frontend\n    command: npm run dev\n    workdir: ./frontend\n    port: 3000\n\n  - name: backend\n    command: python app.py\n    workdir: ./backend\n    port: 5000\n    env:\n      DEBUG: \"true\"\n      DATABASE_URL: \"sqlite:///dev.db\"\n\n  - name: redis\n    command: redis-server\n    port: 6379\nEOF\n\n# Start entire environment\n./pm load-config dev-env.yaml\n./pm start --all\n```\n\n## Future Direction and Roadmap\n\n### Kubernetes Integration\n\nPlanned integration with Kubernetes for hybrid cloud deployments:\n\n```python\nclass KubernetesAdapter:\n    def __init__(self, kubeconfig_path: str):\n        config.load_kube_config(config_file=kubeconfig_path)\n        self.v1 = client.CoreV1Api()\n        self.apps_v1 = client.AppsV1Api()\n\n    def deploy_as_pod(self, process_config: ProcessConfig):\n        pod = client.V1Pod(\n            metadata=client.V1ObjectMeta(name=process_config.name),\n            spec=client.V1PodSpec(\n                containers=[\n                    client.V1Container(\n                        name=process_config.name,\n                        image=process_config.docker_image,\n                        env=[\n                            client.V1EnvVar(name=k, value=v)\n                            for k, v in process_config.env.items()\n                        ]\n                    )\n                ]\n            )\n        )\n        return self.v1.create_namespaced_pod(\n            namespace=\"default\",\n            body=pod\n        )\n```\n\n### Machine Learning for Predictive Scaling\n\nImplementation of predictive scaling based on historical data:\n\n```python\nclass PredictiveScaler:\n    def __init__(self, history_days: int = 30):\n        self.history_days = history_days\n        self.model = None\n\n    def train_model(self, metrics_history: pd.DataFrame):\n        # Feature preparation\n        features = self._extract_features(metrics_history)\n\n        # Training Random Forest model\n        from sklearn.ensemble import RandomForestRegressor\n        self.model = RandomForestRegressor(n_estimators=100)\n        self.model.fit(\n            features[['hour', 'day_of_week', 'cpu_avg', 'memory_avg']],\n            features['optimal_instances']\n        )\n\n    def predict_scaling_needs(self, current_time: datetime) -> int:\n        features = {\n            'hour': current_time.hour,\n            'day_of_week': current_time.weekday(),\n            'cpu_avg': self.get_current_cpu_avg(),\n            'memory_avg': self.get_current_memory_avg()\n        }\n        return int(self.model.predict([list(features.values())])[0])\n```\n\n### GraphQL API\n\nImplementation of GraphQL API for advanced querying:\n\n```python\nimport graphene\n\nclass ProcessType(graphene.ObjectType):\n    id = graphene.String()\n    name = graphene.String()\n    state = graphene.String()\n    cpu_percent = graphene.Float()\n    memory_mb = graphene.Float()\n    uptime_seconds = graphene.Int()\n\nclass Query(graphene.ObjectType):\n    processes = graphene.List(\n        ProcessType,\n        state=graphene.String(),\n        name_contains=graphene.String()\n    )\n\n    def resolve_processes(self, info, state=None, name_contains=None):\n        processes = registry.get_all_processes()\n\n        if state:\n            processes = [p for p in processes if p.state == state]\n\n        if name_contains:\n            processes = [p for p in processes\n                        if name_contains in p.config.name]\n\n        return processes\n\nschema = graphene.Schema(query=Query)\n```\n\n## Performance Metrics and Benchmarks\n\n### Real Production Results\n\nTested on Ubuntu 22.04 LTS, Intel i7-10700K, 32GB RAM:\n\n| Metric | Value | vs PM2 |\n|--------|-------|--------|\n| Memory footprint (idle) | 45 MB | -60% |\n| Process start time | 0.3s | -40% |\n| CPU usage with 100 processes | 2% | -50% |\n| SQLite query (1000 processes) | 2ms | N/A |\n| Health check latency | 5ms | -30% |\n| Max process count | 5000+ | Comparable |\n\n### Performance Optimization in Numbers\n\n```python\n# Process registration benchmark\n# 1000 registrations: 1.24s (1.24ms per operation)\n# 10000 queries: 0.18s (0.018ms per query)\n\n# Memory profiling\n# Base footprint: 45MB\n# Per process overhead: ~2MB\n# With 100 processes: 245MB total\n```\n\n### Load Testing Results\n\n```bash\n# Simulating 100 processes for 24 hours\nUptime: 100%\nFailed restarts: 0\nMemory leaks: None detected\nSQLite fragmentation: < 5%\n```\n\n## Real Deployment Example\n\n### CLI Output in Practice\n\n```bash\n$ ./pm status\n                                Process Status\n┏━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┓\n┃ Name       ┃ State   ┃ PID   ┃ Restarts ┃ CPU % ┃ Memory MB ┃ Uptime ┃ Health ┃\n┡━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━┩\n│ api-server │ running │ 12453 │ 0        │ 1.2   │ 156.3     │ 48h 3m │ ✓      │\n│ worker-1   │ running │ 12486 │ 2        │ 15.6  │ 243.1     │ 47h 5m │ ✓      │\n│ worker-2   │ running │ 12502 │ 1        │ 14.9  │ 238.7     │ 47h 2m │ ✓      │\n│ scheduler  │ running │ 14201 │ 0        │ 0.3   │ 89.2      │ 36h 1m │ ✓      │\n│ redis      │ running │ 11234 │ 0        │ 2.1   │ 456.8     │ 48h 3m │ ✓      │\n└────────────┴─────────┴───────┴──────────┴───────┴───────────┴────────┴────────┘\n```\n\n## Conclusion\n\nUniversal Process Manager represents a robust and flexible solution for process management in modern applications. Thanks to its thoughtful architecture, emphasis on separation of responsibilities, and use of proven technologies, it provides a reliable foundation for managing applications of various types and sizes.\n\nKey advantages of the system include:\n- Complete separation from managed applications\n- Universal support for different process types\n- Robust monitoring and health checking\n- Flexible restart policies\n- Extensibility through plugin system\n- Excellent performance through optimizations\n\nThe project demonstrates best practices in software engineering including clean architecture, SOLID principles, comprehensive testing, and emphasis on security. The project roadmap shows a clear vision for further development towards supporting distributed systems and cloud-native environments.\n\n## 🚀 Call to Action\n\n### Try Universal Process Manager\n\n1. **Clone the repository**\n   ```bash\n   git clone https://github.com/hezky/lab_heartbeat.git\n   cd lab_heartbeat\n   ./setup.sh\n   ```\n\n2. **Contribute to the project**\n   - 🐛 Report bugs or suggest improvements in [Issues](https://github.com/hezky/lab_heartbeat/issues)\n   - 🔧 Send Pull Requests with your enhancements\n   - ⭐ Star the project if you like it\n   - 📖 Improve the documentation\n\n3. **Follow the project**\n   - Subscribe to updates on GitHub\n   - Watch the roadmap for future features\n   - Join the discussion in Issues\n\n### Contact\n\n- **GitHub**: [github.com/hezky/lab_heartbeat](https://github.com/hezky/lab_heartbeat)\n- **Author**: Available through GitHub Issues\n- **License**: MIT - freely usable for commercial projects\n\n---\n\n*If this article helped you, consider sharing it with your community. Happy process managing! 🎉*",
  "metadata": {
    "author": "Mini Blog Team",
    "readingTime": "15 min"
  }
}